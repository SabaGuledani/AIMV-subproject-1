{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0530cf7-be8f-417d-ae19-2f0b7c3c23ea",
   "metadata": {},
   "source": [
    "# Start\n",
    "these libraries are neccesary to install to use this code. specific versions must be installed. just run the code below for the first time when you will run this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5794efd-38da-4e94-aa8a-87ad915d18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pillow==10.4.0\n",
    "# !pip install tensorflow==2.8.3\n",
    "# !pip install ultralytics==8.3.23\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install opencv-python==4.10.0.84\n",
    "# !pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c722543-a20d-4fda-9931-9461f2e6892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomCrop, RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras import backend as K\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from tkinter import simpledialog, filedialog\n",
    "import pytesseract \n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Clear last session to feee up space before new\n",
    "K.clear_session()\n",
    "\n",
    "# if GPU is available this code will state 1\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# image height and width for input into Siamese Network with MobileNetV3 as a base \n",
    "IMG_HEIGHT = 300\n",
    "IMG_WIDTH = 300\n",
    "# important for tesseract to work in jupyter notebook\n",
    "# PATH is relative chooose the one where tesseract.exe is installed\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5fa42f-07fc-44e2-a40f-35f4c56092c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# AI code part for object detection and id card recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87744d82-8f01-420c-82a8-0513700c7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Siamese network model file\n",
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "# Preprocessing function to resize and normalize image\n",
    "def preprocess_image(image):\n",
    "    # resize image to match height and width specified earlier \n",
    "    image_resized = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    \n",
    "    # Normalize values of pixels to be around 0.0-1.0\n",
    "    image_resized = image_resized / 255.0  \n",
    "    \n",
    "    return image_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2cadd-d7b3-443e-9aba-5fc5a79e95ec",
   "metadata": {},
   "source": [
    "## Id cad type prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51bd57ed-f88e-448b-bfce-8e14b644266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese network prediction function  \n",
    "def predict(id_images, img_to_evaluate, model, threshold = 0.65):\n",
    "    '''\n",
    "    id_images - batch of saved id images, should be numpy array of shape (x, 300, 300, 3) where x is number of saved images \n",
    "    or path for image which is saved in the memory and should be compared to, in this case it is string path\n",
    "    img_to_evaluate - image which will be compared to img_base which is saved in the memory\n",
    "    model - AI model file which contains trained weights for siamese network\n",
    "    threshold - optional value which we can state for siamese networks confidence score from 0.50 till 0.99, which means \n",
    "    that results will be shown only if confidence score for similarity is more than threshold\n",
    "    '''\n",
    "    \n",
    "    # preprocess images\n",
    "    if type(id_images) == str:\n",
    "        img_base = np.array(preprocess_image(id_images))\n",
    "        img_base = np.expand_dims(img_base, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    elif isinstance(id_images, np.ndarray):\n",
    "        img_base = id_images\n",
    "\n",
    "    \n",
    "    img_to_evaluate = np.array(preprocess_image(img_to_evaluate))\n",
    "\n",
    "    # expand dimensions of images to add batch dimension \n",
    "    \n",
    "    img_to_evaluate = np.expand_dims(img_to_evaluate, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    \n",
    "    img_to_evaluate = np.tile(img_to_evaluate, (len(img_base), 1, 1, 1))\n",
    "    \n",
    "    # predict the similarity between the two images,\n",
    "    # returns tensor with two values like this [0.37, 0.63] which states how similar are images\n",
    "    # 0.0 no similarity , 1.0 full similarity\n",
    "    preds = model.predict((img_base,img_to_evaluate))\n",
    "    predicted_idx = np.argmax(preds[:, 1])\n",
    "\n",
    "    if preds[predicted_idx][1] > threshold:\n",
    "        return predicted_idx, preds[predicted_idx][1]\n",
    "    else:\n",
    "        return 999,999\n",
    "    # if the model is sure that shown images are more similar than threshold it will output the 1, if not 0\n",
    "    # if predictions are less than a threshold it will output 2   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78faf1-1e67-4345-b0e4-1b702dee5834",
   "metadata": {},
   "source": [
    "### Load models for Object detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aaa0955-860a-436d-921f-eb6d80b6f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Saba/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-23 Python-3.9.0 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load the siamese network model\n",
    "model_name = f'MobileNetV3_1024_siamese.h5'\n",
    "model = load_model(model_name)\n",
    "# load the YOLO model \n",
    "yolo_model = torch.hub.load('ultralytics/yolov5',\"custom\", path=r'.\\yolov5\\runs\\train\\exp6\\weights/best.pt',force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e51b31-05a5-4ce7-9540-a548550f6a65",
   "metadata": {},
   "source": [
    "## YOLO object detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cddec8c6-3b4a-4944-9808-293ab073cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns YOLO model object detection results \n",
    "def yolo(yolo_model,frame):\n",
    "    results = yolo_model(frame)\n",
    "    return results\n",
    "\n",
    "# unpack the coordinates of bounding boxes and confidence score for ID class\n",
    "def get_results_of_yolo(frame_predictions):\n",
    "    x1 = int(frame_predictions.xyxy[0][0][0])\n",
    "    y1 = int(frame_predictions.xyxy[0][0][1])\n",
    "    x2 = int(frame_predictions.xyxy[0][0][2])\n",
    "    y2 = int(frame_predictions.xyxy[0][0][3])\n",
    "    \n",
    "    confidence = frame_predictions.xyxy[0][0][4]\n",
    "    \n",
    "    return x1,y1,x2,y2,confidence\n",
    "\n",
    "# crop the id card from the frame for it to be saved or passed to siamese network for evaluating\n",
    "def crop_id_card(frame_predictions, frame):\n",
    "    x1,y1,x2,y2,confidence = get_results_of_yolo(frame_predictions)\n",
    "    # x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence   class\n",
    "    cropped_image = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41369fd2-b0df-4a1e-9b07-b478d32caeb3",
   "metadata": {},
   "source": [
    "### Id card recognition function\n",
    "we need this function to retrieve already saved ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60cbf638-26d9-42fc-a30a-7cc7f6bfb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path for id_examples\n",
    "id_examples_filepath = './id_examples/'\n",
    "def retrieve_saved_ids():\n",
    "    saved_ids_labels = []\n",
    "    saved_ids_images = []\n",
    "    for id_img in os.listdir(id_examples_filepath):\n",
    "        id_image_uncropped = cv2.imread(id_examples_filepath+id_img)\n",
    "        bounding_box_prediction = yolo(yolo_model,id_image_uncropped)\n",
    "        \n",
    "        # make siamese network work only in the case of finding id card shown\n",
    "        if bounding_box_prediction.xyxy[0].shape != (0,6):\n",
    "            # get coordinates of bounding box\n",
    "            x1,y1,x2,y2,confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "            id_image_cropped = crop_id_card(bounding_box_prediction, id_image_uncropped)\n",
    "            saved_ids_images.append(id_image_cropped)\n",
    "        else: \n",
    "            saved_ids_images.append(id_image_uncropped)\n",
    "            print(\"uncropped version added\")\n",
    "        saved_ids_labels.append(id_img)\n",
    "\n",
    "    id_img_batch = np.array([preprocess_image(image_array) for image_array in saved_ids_images])\n",
    "    if id_img_batch.size == 0:\n",
    "        id_img_batch = np.ones((1, 300, 300, 3), dtype=np.uint8) * 255\n",
    "        saved_ids_labels.append(\"Nothing\")\n",
    "    return id_img_batch, saved_ids_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146205-58ad-48d6-b160-b218374d8f41",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c18fe-df28-469c-9b7f-34be394961df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_id_to_database(frame, new_id_name):\n",
    "    # Create a directory to store new ID images if it doesnâ€™t exist\n",
    "    os.makedirs(\"./id_examples/\", exist_ok=True)\n",
    "\n",
    "    # Create a unique file name with ID type and current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path = f\"./id_examples/{new_id_name}_{timestamp}.jpg\"\n",
    "\n",
    "    # Save the image to the specified path\n",
    "    cv2.imwrite(file_path, frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa3b08-df6a-4f34-8ab4-5f481db43fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user_for_id_type():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    new_id_type = simpledialog.askstring(\"Input\", \"Enter new ID type name:\")\n",
    "    root.destroy()\n",
    "    return new_id_type\n",
    "    \n",
    "# Create a button that prints the input field's text when clicked\n",
    "def on_button_click(id_image):\n",
    "    new_id_name = input_entry.get()  # Get text from entry field\n",
    "    print(new_id_name + \"es daarqva\")\n",
    "    if new_id_name != None:\n",
    "        add_new_id_to_database(id_image, new_id_name)\n",
    "        id_images_batch, id_labels = retrieve_saved_ids()\n",
    "        input_entry.delete(0, tk.END)  # Clear the entry field after clicking\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_window_submit_window():\n",
    "    window_submit_photo = tk.TK()\n",
    "    window_submit_photo.title(\"ID submission\")\n",
    "    window.geometry(\"1200x800\")\n",
    "    \n",
    "    video_label_submit = tk.Label(window_submit_photo)\n",
    "    video_label_submit.pack()\n",
    "    \n",
    "    input_entry = tk.Entry(window, width=30)\n",
    "    input_entry.pack()\n",
    "\n",
    "    button = tk.Button(window, text=\"Submit\", command=on_button_click)\n",
    "    button.pack()\n",
    "    \n",
    "def check_for_new_id_type(frame,confidence_score):\n",
    "    global frame_counter\n",
    "    if confidence_score > confidence_score_threshold:\n",
    "        if frame_counter < 20:\n",
    "            frame_counter += 1\n",
    "            return False\n",
    "        else:\n",
    "            new_id_name = ask_user_for_id_type()\n",
    "            if new_id_name != None:\n",
    "                add_new_id_to_database(frame, new_id_name)\n",
    "                frame_counter = 0\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5671350-bb85-42b7-a77c-f4934136f686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6c53e-6737-44e6-bc79-25425322cbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2359e27-e67a-431c-a9b6-e8fc14726aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9daab43d-8bf8-41e9-822c-092b2398a29a",
   "metadata": {},
   "source": [
    "# Detecting, segmenting and recognizing descriptive regions in ID cards\n",
    "Part of code for subproject part 2\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def7f02-d340-40a4-a0b7-4e73167cdf61",
   "metadata": {},
   "source": [
    "## Functions checking blurryness and reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918911a-9800-4896-9157-ad3e8b9d22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blurry(image, threshold=100):\n",
    "    \"\"\"\n",
    "    Check if an image is blurry using the variance of the Laplacian.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    threshold (float): Threshold below which the image is considered blurry.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the image is blurry, False otherwise.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return laplacian_var < threshold, laplacian_var\n",
    "\n",
    "def has_reflection(image, bright_threshold=200, reflection_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Check if an image has significant reflections based on bright regions.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    bright_threshold (int): Pixel intensity threshold to consider as bright.\n",
    "    reflection_ratio (float): Proportion of bright pixels to consider as reflection.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the image has high reflection, False otherwise.\n",
    "    float reflection_percentage\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    bright_pixels = cv2.threshold(gray, bright_threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "    bright_pixel_count = np.sum(bright_pixels == 255)\n",
    "    total_pixels = gray.size\n",
    "    reflection_percentage = bright_pixel_count / total_pixels\n",
    "    return reflection_percentage > reflection_ratio, reflection_percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eebaf-2e81-49b0-8ae1-6a2de9319e05",
   "metadata": {},
   "source": [
    "## Function for extracting faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0971d0-b085-44a9-bdfa-e603fce23ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_face(image):\n",
    "    height, width, dim = image.shape\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    for (x, y, w, h) in faces:\n",
    "        if 5 < ((w*h) * 100)/(height * width) < 50:\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822840be-f635-4b3a-bb92-c8a7a7225fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83302e55-c0b3-40ec-abbc-3d5dc707a0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888c096-6244-490a-81f8-6682aedfe971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453d11d-052e-4c67-8b48-d58e7786d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e55ef5d2-dbdc-4926-a0fb-756e2916aa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "937263b6-a1ba-4f4b-b037-84152165132c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03759594-411e-495d-8f86-3423bd5faf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mrz_area(image):\n",
    "    \"\"\"\n",
    "    Find the area where the MRZ is located in an ID card image.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input ID card image.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The bounding box (x, y, w, h) of the MRZ area.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize a rectangular and square structuring kernel (this size is dependent on the ID-Card size)\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 5))\n",
    "    sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 31))\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise and improve contour detection\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    # cv2.imshow('Gray', gray)\n",
    "\n",
    "    # Apply blackhat morphological operation to enhance the text\n",
    "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "    # cv2.imshow('Blackhat', blackhat)\n",
    "\n",
    "    # apply a closing operation using the rectangular kernel to close\n",
    "\t# gaps in between letters -- then apply Otsu's thresholding method\n",
    "    blackhat_closed = cv2.morphologyEx(blackhat, cv2.MORPH_CLOSE, rectKernel)\n",
    "    # cv2.imshow('blackhat_closed', blackhat_closed)\n",
    "    thresh = cv2.threshold(blackhat_closed, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    # cv2.imshow('Thresh', thresh)\n",
    "\n",
    "    # perform another closing operation, this time using the square\n",
    "\t# kernel to close gaps between lines of the MRZ\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)\n",
    "    # cv2.imshow('Thresh_closed', thresh)\n",
    "\n",
    "    # Convert the thresholded image to a color image to plot the contours in color\n",
    "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "\t# find contours in the thresholded image and sort them by their\n",
    "\t# size\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Handle different versions of OpenCV\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    # Sort contours by area in descending order    \n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # Initialize ROI as None\n",
    "    roi = None\n",
    "\n",
    "\t# loop over the contours\n",
    "    for i, c in enumerate(cnts):\n",
    "\t\t# compute the bounding box of the contour and use the contour to\n",
    "\t\t# compute the aspect ratio and coverage ratio of the bounding box\n",
    "\t\t# width to the width of the image\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "         # Draw the bounding box on the color thresholded image\n",
    "        color = (0, 255, 0) if i == 0 else (0, 0, 255)  # Green for the first (largest) contour, red for others\n",
    "        cv2.rectangle(thresh_color, (x, y), (x + w, y + h), color, 2)\n",
    "        # cv2.imshow('Thresholded Image with Contours', thresh_color)\n",
    "\n",
    "        ar = w / float(h)\n",
    "        crWidth = w / float(gray.shape[1])\n",
    "\t\t# check to see if the aspect ratio and coverage width are within\n",
    "\t\t# acceptable criteria\n",
    "        if ar > 4 and crWidth > 0.75:\n",
    "\t\t\t# pad the bounding box to have some space for later reading\n",
    "            pad = 5\n",
    "            x = x - pad\n",
    "            y = y - pad\n",
    "            w = w + (pad * 2)\n",
    "            h = h + (pad * 2)\n",
    "\n",
    "\t\t\t# extract the ROI from the image and draw a bounding box\n",
    "\t\t\t# surrounding the MRZ\n",
    "            roi = image[y:y + h, x:x + w]\n",
    "\n",
    "            break\n",
    "\n",
    "    return (x, y, w, h), roi if roi is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18298de4-c924-469a-967d-b0ad065763fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confidence_score_threshold = 0.7\n",
    "\n",
    "# Open video capture (0 is the default camera)\n",
    "video_path = \"./passport_video.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "id_images_batch, id_labels = retrieve_saved_ids()\n",
    "frame_counter = 0  # Use local variable instead of global\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps == 0:  # Avoid division by zero\n",
    "    fps = 30  # Default to 30 FPS if unable to retrieve\n",
    "frame_delay = int(1000 / fps)  # Calculate frame delay in milliseconds\n",
    "print(frame_delay)\n",
    "\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"ID Detection\")\n",
    "window.geometry(\"1200x800\")\n",
    "\n",
    "\n",
    "# Create a label to display the video feed\n",
    "video_label = tk.Label(window)\n",
    "video_label.pack()\n",
    "\n",
    "# Function to update frame in Tkinter window\n",
    "def update_frame():\n",
    "    global frame_counter, id_images_batch, id_labels\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        window.after(10, update_frame)  # Schedule next frame update\n",
    "        return\n",
    "\n",
    "    \n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "    \n",
    "    # # Predict bounding box coordinates\n",
    "    # bounding_box_prediction = yolo(yolo_model, frame)\n",
    "\n",
    "    # # Make siamese network work only if ID card is found\n",
    "    # if bounding_box_prediction.xyxy[0].shape != (0, 6):\n",
    "    #     # Get coordinates of bounding box\n",
    "    #     x1, y1, x2, y2, confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "\n",
    "    #     # Crop the ID card\n",
    "    #     cropped_id_card = crop_id_card(bounding_box_prediction, frame)\n",
    "\n",
    "    #     # Check if a new ID card is detected\n",
    "\n",
    "    \n",
    "    #     # Predict with siamese network\n",
    "    #     prediction_idx, confidence_score_siamese = predict(id_images_batch, cropped_id_card, model, threshold=0.95)\n",
    "\n",
    "    #     if prediction_idx == 999:\n",
    "    #         prediction_name = \"not found\"\n",
    "    #         if check_for_new_id_type(frame,confidence_score) == True:\n",
    "    #             id_images_batch, id_labels = retrieve_saved_ids()\n",
    "                \n",
    "    #     else:\n",
    "    #         prediction_name = id_labels[prediction_idx]\n",
    "\n",
    "    #     cv2.putText(frame, f'id type: {prediction_name}, {confidence_score_siamese:.2f}', (10, 30),\n",
    "    #                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "    #     if confidence_score > 0.45:\n",
    "    #         # Draw bounding box on the frame\n",
    "    #         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    #         text = f'{confidence_score:.2f}'\n",
    "    #         text_position = (x1, y1 - 10)\n",
    "    #         cv2.putText(frame, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    mrz_area, roi = find_mrz_area(frame)\n",
    "\n",
    "    if mrz_area is not None:\n",
    "        x, y, w, h = mrz_area\n",
    "        print(mrz_area)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        # cv2.imshow('MRZ Area', frame)\n",
    "        if roi is not None:\n",
    "            # print(roi)\n",
    "            if roi.shape[0]>0 and roi.shape[1]>0:\n",
    "                cv2.imshow('MRZ ROI', roi)\n",
    "    else:\n",
    "        print(\"MRZ area not found.\")\n",
    "    # Convert frame to RGB and update the Tkinter label\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    video_label.after(frame_delay, update_frame)\n",
    "\n",
    "# Start updating frames\n",
    "update_frame()\n",
    "\n",
    "# Define a function to handle window close\n",
    "def on_closing():\n",
    "    cap.release()  # Release the camera\n",
    "    window.destroy()  # Close the Tkinter window\n",
    "\n",
    "# Bind the window close event\n",
    "window.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d7594675-7981-455a-8424-9f667395fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./georgian_id_front.png')\n",
    "\n",
    "\n",
    "image = find_face(image)\n",
    "image = cv2.resize(image,(960,540))\n",
    "cv2.imshow(\"lala\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3fe572f5-4fbf-423e-8c73-3ce29b566a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./georgian_id_front.png')\n",
    "\n",
    "rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 47))\n",
    "\n",
    "# Apply blackhat morphology\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "\n",
    "# Threshold and find contours\n",
    "_, thresh = cv2.threshold(blackhat, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# debug_image = image.copy()  # Create a copy of the image for visualization\n",
    "# cv2.drawContours(debug_image, contours, -1, (0, 255, 0), 2)  # Draw all contours in green\n",
    "\n",
    "# # Show the contours\n",
    "# cv2.imshow('Contours', debug_image)\n",
    "# Filter contours for signature area\n",
    "height, width, dim = image.shape\n",
    "for c in contours:\n",
    "    x, y, w, h = cv2.boundingRect(c)\n",
    "    if 1 < ((w*h) * 100)/(height * width) < 5:\n",
    "        aspect_ratio = w / float(h)\n",
    "\n",
    "        if 1.5 < aspect_ratio < 4.5:   # Signature is often wide\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "image = cv2.resize(image,(960,540))\n",
    "cv2.imshow(\"lala\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cff967-4795-440a-b7f5-9effa9e155ca",
   "metadata": {},
   "source": [
    "# blurryness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "005db7f6-1732-4480-b5e6-380fe81e74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"./passport_video.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "def update_frame():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        return\n",
    "    return ret,frame\n",
    "    \n",
    "def sharpen_with_kernel(image):\n",
    "    \"\"\"\n",
    "    Sharpen an image using a convolutional kernel.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input image.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The sharpened image.\n",
    "    \"\"\"\n",
    "    # Define a sharpening kernel\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "\n",
    "    # Apply the kernel to the image\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    \n",
    "    return sharpened\n",
    "\n",
    "    \n",
    "window_ratio = 250\n",
    "while True:\n",
    "    try:\n",
    "        ret,frame = update_frame()\n",
    "    except:\n",
    "        print(\"video finished\")\n",
    "        break\n",
    "    frame = cv2.resize(frame, (4*window_ratio, 3*window_ratio))\n",
    "    bounding_box_prediction = yolo(yolo_model, frame)\n",
    "\n",
    "    # Make siamese network work only if ID card is found\n",
    "    if bounding_box_prediction.xyxy[0].shape != (0, 6):\n",
    "        # Get coordinates of bounding box\n",
    "        x1, y1, x2, y2, confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "\n",
    "        # Crop the ID card\n",
    "        cropped_id_image = crop_id_card(bounding_box_prediction, frame)\n",
    "\n",
    "        if confidence_score > 0.45:\n",
    "            # Draw bounding box on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            text = f'{confidence_score:.2f}'\n",
    "            text_position = (x1, y1 - 10)\n",
    "            cv2.putText(frame, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    result, sharpness = is_blurry(cropped_id_image)\n",
    "    result_reflection, reflection_score = has_reflection(cropped_id_image)\n",
    "    cv2.putText(frame, f'sharpness score: {sharpness:.2f}, {reflection_score:.2%}', (10, 30),\n",
    "              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "    cv2.imshow(\"frames\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the capture and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b08fc21-33dc-4ee0-aa89-a091b4a19317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image):\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2897a-a0e3-41b2-a281-61db6a7645fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bc85f-4d57-4580-8c63-a621c66e75ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ed331-1bbb-42a6-bce2-e2ee0d3b7431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
