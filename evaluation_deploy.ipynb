{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0530cf7-be8f-417d-ae19-2f0b7c3c23ea",
   "metadata": {},
   "source": [
    "# Start\n",
    "these libraries are neccesary to install to use this code. specific versions must be installed. just run the code below for the first time when you will run this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5794efd-38da-4e94-aa8a-87ad915d18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pillow==10.4.0\n",
    "# !pip install tensorflow==2.8.3\n",
    "# !pip install ultralytics==8.3.23\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install opencv-python==4.10.0.84\n",
    "# !pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c722543-a20d-4fda-9931-9461f2e6892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomCrop, RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras import backend as K\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from tkinter import simpledialog, filedialog\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Clear last session to feee up space before new\n",
    "K.clear_session()\n",
    "\n",
    "# if GPU is available this code will state 1\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# image height and width for input into Siamese Network with MobileNetV3 as a base \n",
    "IMG_HEIGHT = 300\n",
    "IMG_WIDTH = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87744d82-8f01-420c-82a8-0513700c7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Siamese network model file\n",
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "# Preprocessing function to resize and normalize image\n",
    "def preprocess_image(image):\n",
    "    # resize image to match height and width specified earlier \n",
    "    image_resized = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    \n",
    "    # Normalize values of pixels to be around 0.0-1.0\n",
    "    image_resized = image_resized / 255.0  \n",
    "    \n",
    "    return image_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bd57ed-f88e-448b-bfce-8e14b644266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese network prediction function  \n",
    "def predict(id_images, img_to_evaluate, model, threshold = 0.65):\n",
    "    '''\n",
    "    id_images_batch - batch of saved id images, should be numpy array of shape (x, 300, 300, 3) where x is number of saved images \n",
    "    or path for image which is saved in the memory and should be compared to, in this case it is string path\n",
    "    img_to_evaluate - image which will be compared to img_base which is saved in the memory\n",
    "    model - AI model file which contains trained weights for siamese network\n",
    "    threshold - optional value which we can state for siamese networks confidence score from 0.50 till 0.99, which means \n",
    "    that results will be shown only if confidence score for similarity is more than threshold\n",
    "    '''\n",
    "    \n",
    "    # preprocess images\n",
    "    if type(id_images) == str:\n",
    "        img_base = np.array(preprocess_image(id_images))\n",
    "        img_base = np.expand_dims(img_base, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    elif isinstance(id_images, np.ndarray):\n",
    "        img_base = id_images\n",
    "\n",
    "    \n",
    "    img_to_evaluate = np.array(preprocess_image(img_to_evaluate))\n",
    "\n",
    "    # expand dimensions of images to add batch dimension \n",
    "    \n",
    "    img_to_evaluate = np.expand_dims(img_to_evaluate, axis=0)  # Add batch dimension, shape becomes (1, 300, 300, 3)\n",
    "    \n",
    "    img_to_evaluate = np.tile(img_to_evaluate, (len(img_base), 1, 1, 1))\n",
    "    \n",
    "    # predict the similarity between the two images,\n",
    "    # returns tensor with two values like this [0.37, 0.63] which states how similar are images\n",
    "    # 0.0 no similarity , 1.0 full similarity\n",
    "    preds = model.predict((img_base,img_to_evaluate))\n",
    "    predicted_idx = np.argmax(preds[:, 1])\n",
    "\n",
    "    if preds[predicted_idx][1] > threshold:\n",
    "        return predicted_idx, preds[predicted_idx][1]\n",
    "    else:\n",
    "        return 999,999\n",
    "    # if the model is sure that shown images are more similar than threshold it will output the 1, if not 0\n",
    "    # if predictions are less than a threshold it will output 2   \n",
    "    \n",
    "    # if preds[0][0] > threshold:\n",
    "    #     predicted_idx = 0\n",
    "    # elif preds[0][1] > threshold:\n",
    "    #     predicted_idx = 1\n",
    "    # else:\n",
    "    #     predicted_idx = 2\n",
    "    # predicted_idx = np.argmax(preds)\n",
    "    # funciton also returns the predictions list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aaa0955-860a-436d-921f-eb6d80b6f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Saba/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-23 Python-3.9.0 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load the siamese network model\n",
    "model_name = f'MobileNetV3_1024_siamese.h5'\n",
    "model = load_model(model_name)\n",
    "# load the YOLO model \n",
    "yolo_model = torch.hub.load('ultralytics/yolov5',\"custom\", path=r'.\\yolov5\\runs\\train\\exp6\\weights/best.pt',force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96227c48-ff49-424c-bbcb-0c2bf87ad05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cddec8c6-3b4a-4944-9808-293ab073cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns YOLO model object detection results \n",
    "def yolo(yolo_model,frame):\n",
    "    results = yolo_model(frame)\n",
    "    return results\n",
    "\n",
    "# unpack the coordinates of bounding boxes and confidence score for ID class\n",
    "def get_results_of_yolo(frame_predictions):\n",
    "    x1 = int(frame_predictions.xyxy[0][0][0])\n",
    "    y1 = int(frame_predictions.xyxy[0][0][1])\n",
    "    x2 = int(frame_predictions.xyxy[0][0][2])\n",
    "    y2 = int(frame_predictions.xyxy[0][0][3])\n",
    "    \n",
    "    confidence = frame_predictions.xyxy[0][0][4]\n",
    "    \n",
    "    return x1,y1,x2,y2,confidence\n",
    "\n",
    "# crop the id card from the frame for it to be saved or passed to siamese network for evaluating\n",
    "def crop_id_card(frame_predictions, frame):\n",
    "    x1,y1,x2,y2,confidence = get_results_of_yolo(frame_predictions)\n",
    "    # x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence   class\n",
    "    cropped_image = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60cbf638-26d9-42fc-a30a-7cc7f6bfb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path for id_examples\n",
    "id_examples_filepath = './id_examples/'\n",
    "def retrieve_saved_ids():\n",
    "    saved_ids_labels = []\n",
    "    saved_ids_images = []\n",
    "    for id_img in os.listdir(id_examples_filepath):\n",
    "        id_image_uncropped = cv2.imread(id_examples_filepath+id_img)\n",
    "        bounding_box_prediction = yolo(yolo_model,id_image_uncropped)\n",
    "        \n",
    "        # make siamese network work only in the case of finding id card shown\n",
    "        if bounding_box_prediction.xyxy[0].shape != (0,6):\n",
    "            # get coordinates of bounding box\n",
    "            x1,y1,x2,y2,confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "            id_image_cropped = crop_id_card(bounding_box_prediction, id_image_uncropped)\n",
    "            saved_ids_images.append(id_image_cropped)\n",
    "        else: \n",
    "            saved_ids_images.append(id_image_uncropped)\n",
    "            print(\"uncropped version added\")\n",
    "        saved_ids_labels.append(id_img)\n",
    "\n",
    "    id_img_batch = np.array([preprocess_image(image_array) for image_array in saved_ids_images])\n",
    "    if id_img_batch.size == 0:\n",
    "        id_img_batch = np.ones((1, 300, 300, 3), dtype=np.uint8) * 255\n",
    "        saved_ids_labels.append(\"Nothing\")\n",
    "    return id_img_batch, saved_ids_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918911a-9800-4896-9157-ad3e8b9d22c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55ef5d2-dbdc-4926-a0fb-756e2916aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_id_to_database(frame, new_id_name):\n",
    "    # Create a directory to store new ID images if it doesn’t exist\n",
    "    os.makedirs(\"./id_examples/\", exist_ok=True)\n",
    "\n",
    "    # Create a unique file name with ID type and current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path = f\"./id_examples/{new_id_name}_{timestamp}.jpg\"\n",
    "\n",
    "    # Save the image to the specified path\n",
    "    cv2.imwrite(file_path, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "937263b6-a1ba-4f4b-b037-84152165132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_still_frame(current_frame, previous_frame, threshold=50000):\n",
    "#     # Calculate the absolute difference between frames\n",
    "#     diff = cv2.absdiff(previous_frame, current_frame)\n",
    "#     # Convert to grayscale and compute the sum of differences\n",
    "#     gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "#     non_zero_count = np.sum(gray_diff)\n",
    "#     return non_zero_count < threshold\n",
    "\n",
    "# def is_sharp_frame(frame, threshold=50):\n",
    "#     # Calculate the Laplacian to assess sharpness (higher variance = sharper)\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     variance = cv2.Laplacian(gray_frame, cv2.CV_64F).var()\n",
    "#     return variance > threshold\n",
    "\n",
    "def ask_user_for_id_type():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    new_id_type = simpledialog.askstring(\"Input\", \"Enter new ID type name:\")\n",
    "    root.destroy()\n",
    "    return new_id_type\n",
    "    \n",
    "# Create a button that prints the input field's text when clicked\n",
    "def on_button_click(id_image):\n",
    "    new_id_name = input_entry.get()  # Get text from entry field\n",
    "    print(new_id_name + \"es daarqva\")\n",
    "    if new_id_name != None:\n",
    "        add_new_id_to_database(id_image, new_id_name)\n",
    "        id_images_batch, id_labels = retrieve_saved_ids()\n",
    "        input_entry.delete(0, tk.END)  # Clear the entry field after clicking\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_window_submit_window():\n",
    "    window_submit_photo = tk.TK()\n",
    "    window_submit_photo.title(\"ID submission\")\n",
    "    window.geometry(\"1200x800\")\n",
    "    \n",
    "    video_label_submit = tk.Label(window_submit_photo)\n",
    "    video_label_submit.pack()\n",
    "    \n",
    "    input_entry = tk.Entry(window, width=30)\n",
    "    input_entry.pack()\n",
    "\n",
    "    button = tk.Button(window, text=\"Submit\", command=on_button_click)\n",
    "    button.pack()\n",
    "    \n",
    "def check_for_new_id_type(frame,confidence_score):\n",
    "    global frame_counter\n",
    "    if confidence_score > confidence_score_threshold:\n",
    "        if frame_counter < 20:\n",
    "            frame_counter += 1\n",
    "            return False\n",
    "        else:\n",
    "            new_id_name = ask_user_for_id_type()\n",
    "            if new_id_name != None:\n",
    "                add_new_id_to_database(frame, new_id_name)\n",
    "                frame_counter = 0\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03759594-411e-495d-8f86-3423bd5faf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mrz_area(image):\n",
    "    \"\"\"\n",
    "    Find the area where the MRZ is located in an ID card image.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input ID card image.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The bounding box (x, y, w, h) of the MRZ area.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize a rectangular and square structuring kernel (this size is dependent on the ID-Card size)\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 5))\n",
    "    sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 31))\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise and improve contour detection\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    # cv2.imshow('Gray', gray)\n",
    "\n",
    "    # Apply blackhat morphological operation to enhance the text\n",
    "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "    # cv2.imshow('Blackhat', blackhat)\n",
    "\n",
    "    # apply a closing operation using the rectangular kernel to close\n",
    "\t# gaps in between letters -- then apply Otsu's thresholding method\n",
    "    blackhat_closed = cv2.morphologyEx(blackhat, cv2.MORPH_CLOSE, rectKernel)\n",
    "    # cv2.imshow('blackhat_closed', blackhat_closed)\n",
    "    thresh = cv2.threshold(blackhat_closed, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    # cv2.imshow('Thresh', thresh)\n",
    "\n",
    "    # perform another closing operation, this time using the square\n",
    "\t# kernel to close gaps between lines of the MRZ\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)\n",
    "    # cv2.imshow('Thresh_closed', thresh)\n",
    "\n",
    "    # Convert the thresholded image to a color image to plot the contours in color\n",
    "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "\t# find contours in the thresholded image and sort them by their\n",
    "\t# size\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Handle different versions of OpenCV\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    # Sort contours by area in descending order    \n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # Initialize ROI as None\n",
    "    roi = None\n",
    "\n",
    "\t# loop over the contours\n",
    "    for i, c in enumerate(cnts):\n",
    "\t\t# compute the bounding box of the contour and use the contour to\n",
    "\t\t# compute the aspect ratio and coverage ratio of the bounding box\n",
    "\t\t# width to the width of the image\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "         # Draw the bounding box on the color thresholded image\n",
    "        color = (0, 255, 0) if i == 0 else (0, 0, 255)  # Green for the first (largest) contour, red for others\n",
    "        cv2.rectangle(thresh_color, (x, y), (x + w, y + h), color, 2)\n",
    "        # cv2.imshow('Thresholded Image with Contours', thresh_color)\n",
    "\n",
    "        ar = w / float(h)\n",
    "        crWidth = w / float(gray.shape[1])\n",
    "\t\t# check to see if the aspect ratio and coverage width are within\n",
    "\t\t# acceptable criteria\n",
    "        if ar > 4 and crWidth > 0.75:\n",
    "\t\t\t# pad the bounding box to have some space for later reading\n",
    "            pad = 5\n",
    "            x = x - pad\n",
    "            y = y - pad\n",
    "            w = w + (pad * 2)\n",
    "            h = h + (pad * 2)\n",
    "\n",
    "\t\t\t# extract the ROI from the image and draw a bounding box\n",
    "\t\t\t# surrounding the MRZ\n",
    "            roi = image[y:y + h, x:x + w]\n",
    "\n",
    "            break\n",
    "\n",
    "    return (x, y, w, h), roi if roi is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18298de4-c924-469a-967d-b0ad065763fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "(416, 135, 4, 1)\n",
      "(331, 338, 1, 1)\n",
      "(355, 111, 1, 2)\n",
      "(238, 332, 3, 1)\n",
      "(339, 300, 1, 1)\n",
      "(0, 306, 8, 1)\n",
      "(316, 142, 2, 1)\n",
      "(169, 180, 4, 1)\n",
      "(454, 303, 1, 1)\n",
      "(89, 24, 1, 1)\n",
      "(335, 145, 1, 2)\n",
      "(359, 54, 2, 1)\n",
      "(314, 176, 1, 1)\n",
      "(448, 280, 1, 1)\n",
      "(68, 131, 1, 1)\n",
      "(68, 131, 1, 1)\n",
      "(564, 310, 2, 1)\n",
      "(790, 243, 10, 1)\n",
      "(539, 342, 1, 1)\n",
      "(695, 461, 1, 1)\n",
      "(157, 179, 3, 1)\n",
      "(365, 369, 2, 1)\n",
      "(135, 175, 5, 1)\n",
      "(346, 385, 4, 1)\n",
      "(298, 354, 1, 1)\n",
      "(414, 290, 2, 1)\n",
      "(443, 249, 1, 1)\n",
      "(220, 292, 2, 1)\n",
      "(464, 181, 9, 1)\n",
      "(466, 163, 1, 1)\n",
      "(596, 363, 2, 2)\n",
      "(323, 476, 1, 1)\n",
      "(359, 474, 5, 1)\n",
      "(106, 495, 3, 2)\n",
      "(755, 241, 8, 1)\n",
      "(59, 96, 2, 1)\n",
      "(365, 284, 1, 1)\n",
      "(315, 19, 5, 1)\n",
      "(605, 66, 4, 1)\n",
      "(628, 0, 1, 12)\n",
      "(551, 0, 1, 9)\n",
      "(120, 378, 644, 85)\n",
      "(121, 380, 644, 84)\n",
      "(121, 397, 650, 74)\n",
      "(120, 400, 653, 72)\n",
      "(115, 401, 652, 70)\n",
      "(113, 400, 652, 69)\n",
      "(110, 398, 652, 69)\n",
      "(107, 397, 652, 68)\n",
      "(101, 388, 648, 70)\n",
      "(100, 387, 648, 69)\n",
      "(100, 384, 647, 71)\n",
      "(404, 44, 1, 1)\n",
      "(409, 54, 1, 1)\n",
      "(431, 54, 1, 2)\n",
      "(66, 375, 646, 73)\n",
      "(64, 375, 647, 72)\n",
      "(62, 376, 645, 70)\n",
      "(58, 377, 639, 66)\n",
      "(577, 30, 2, 1)\n",
      "(589, 35, 3, 1)\n",
      "(45, 79, 1, 1)\n",
      "(-5, 385, 646, 73)\n",
      "(-5, 387, 647, 72)\n",
      "(-5, 388, 646, 73)\n",
      "(-5, 389, 646, 72)\n",
      "(-5, 383, 638, 74)\n",
      "(-5, 383, 634, 73)\n",
      "(-5, 385, 632, 71)\n",
      "(-5, 387, 632, 64)\n",
      "(348, 59, 1, 1)\n",
      "(433, 60, 1, 1)\n",
      "(203, 35, 2, 1)\n",
      "(553, 19, 2, 1)\n",
      "(296, 183, 1, 1)\n",
      "(239, 159, 2, 1)\n",
      "(263, 16, 1, 1)\n",
      "(564, 164, 2, 1)\n",
      "(275, 21, 1, 1)\n",
      "(178, 40, 4, 1)\n",
      "(614, 174, 1, 1)\n",
      "(300, 20, 5, 1)\n",
      "(280, 22, 2, 1)\n",
      "(99, 43, 4, 1)\n",
      "(129, 269, 1, 1)\n",
      "(190, 194, 1, 1)\n",
      "(311, 456, 3, 1)\n",
      "(298, 210, 1, 1)\n",
      "(799, 85, 1, 1)\n",
      "(134, 91, 1, 1)\n",
      "(-5, 419, 631, 96)\n",
      "(-5, 418, 629, 100)\n",
      "(-5, 417, 628, 104)\n",
      "(-5, 413, 630, 99)\n",
      "(-5, 414, 635, 97)\n",
      "(6, 415, 629, 96)\n",
      "(103, 0, 10, 1)\n",
      "(296, 126, 2, 1)\n",
      "(87, 19, 3, 1)\n",
      "(129, 0, 1, 1)\n",
      "(55, 0, 1, 10)\n",
      "(190, 0, 1, 1)\n",
      "(338, 48, 1, 1)\n",
      "(594, 58, 4, 1)\n",
      "(625, 64, 1, 1)\n",
      "(67, 40, 1, 22)\n",
      "(66, 43, 1, 2)\n",
      "(145, 0, 1, 1)\n",
      "(248, 78, 2, 1)\n",
      "(576, 120, 1, 1)\n",
      "(405, 85, 1, 2)\n",
      "(620, 121, 1, 2)\n",
      "(419, 90, 2, 1)\n",
      "(576, 117, 1, 2)\n",
      "(282, 45, 1, 1)\n",
      "(261, 27, 1, 1)\n",
      "(619, 100, 1, 1)\n",
      "(397, 73, 2, 1)\n",
      "(568, 90, 2, 1)\n",
      "(395, 78, 3, 1)\n",
      "(421, 79, 3, 1)\n",
      "(77, 389, 611, 80)\n",
      "(76, 125, 2, 2)\n",
      "(75, 388, 611, 79)\n",
      "(71, 380, 616, 85)\n",
      "(72, 378, 617, 88)\n",
      "(73, 373, 614, 94)\n",
      "(73, 371, 622, 97)\n",
      "(67, 361, 627, 117)\n",
      "(65, 362, 628, 118)\n",
      "(63, 362, 630, 118)\n",
      "(61, 361, 631, 118)\n",
      "(53, 355, 631, 119)\n",
      "(52, 354, 631, 119)\n",
      "(52, 355, 630, 118)\n",
      "(52, 357, 629, 116)\n",
      "(52, 356, 630, 117)\n",
      "(53, 356, 629, 118)\n",
      "(57, 362, 635, 126)\n",
      "(56, 364, 637, 128)\n",
      "(56, 365, 637, 132)\n",
      "(56, 369, 634, 132)\n",
      "(47, 376, 641, 126)\n",
      "(42, 376, 642, 121)\n",
      "(38, 376, 643, 119)\n",
      "(20, 392, 637, 95)\n",
      "(16, 397, 643, 89)\n",
      "(13, 403, 642, 83)\n",
      "(12, 406, 642, 79)\n",
      "(11, 413, 641, 76)\n",
      "(11, 415, 641, 76)\n",
      "(11, 416, 641, 76)\n",
      "(16, 421, 640, 76)\n",
      "(16, 421, 641, 75)\n",
      "(18, 419, 640, 76)\n",
      "(29, 415, 638, 78)\n",
      "(34, 413, 637, 80)\n",
      "(38, 413, 634, 79)\n",
      "(42, 412, 634, 80)\n",
      "(57, 411, 632, 81)\n",
      "(60, 413, 633, 81)\n",
      "(60, 413, 633, 81)\n",
      "(62, 415, 633, 83)\n",
      "(63, 418, 633, 81)\n",
      "(64, 418, 634, 81)\n",
      "(70, 420, 631, 79)\n",
      "(72, 419, 629, 80)\n",
      "(74, 420, 629, 80)\n",
      "(76, 422, 629, 77)\n",
      "(81, 433, 626, 72)\n",
      "(80, 441, 627, 68)\n",
      "(80, 447, 627, 66)\n",
      "(80, 450, 626, 65)\n",
      "(79, 454, 625, 63)\n",
      "(80, 464, 624, 63)\n",
      "(80, 464, 624, 63)\n",
      "(81, 469, 623, 62)\n",
      "(81, 469, 623, 62)\n",
      "(79, 469, 625, 63)\n",
      "(78, 470, 625, 63)\n",
      "(78, 469, 626, 64)\n",
      "(77, 469, 626, 63)\n",
      "(77, 467, 626, 65)\n",
      "(80, 461, 625, 65)\n",
      "(81, 460, 624, 64)\n",
      "(82, 460, 623, 64)\n",
      "(82, 460, 623, 64)\n",
      "(77, 460, 624, 66)\n",
      "(76, 462, 624, 66)\n",
      "(74, 463, 625, 66)\n",
      "(70, 465, 626, 65)\n",
      "(70, 464, 626, 66)\n",
      "(69, 464, 627, 66)\n",
      "(69, 464, 626, 66)\n",
      "(67, 463, 627, 65)\n",
      "(66, 462, 627, 65)\n",
      "(65, 461, 626, 66)\n",
      "(63, 460, 627, 67)\n",
      "(61, 461, 628, 66)\n",
      "(59, 462, 629, 67)\n",
      "(58, 465, 629, 66)\n",
      "(58, 466, 629, 65)\n",
      "(56, 466, 631, 66)\n",
      "(55, 466, 630, 66)\n",
      "(54, 465, 629, 66)\n",
      "(53, 465, 629, 66)\n",
      "(53, 465, 628, 65)\n",
      "(52, 464, 628, 66)\n",
      "(51, 466, 628, 65)\n",
      "(52, 466, 627, 65)\n",
      "(52, 466, 627, 65)\n",
      "(53, 467, 628, 66)\n",
      "(55, 468, 627, 66)\n",
      "(55, 468, 627, 66)\n",
      "(53, 468, 628, 66)\n",
      "(53, 467, 627, 66)\n",
      "(50, 467, 628, 65)\n",
      "(46, 466, 626, 64)\n",
      "(0, 319, 8, 16)\n",
      "(606, 0, 2, 2)\n",
      "(606, 0, 2, 2)\n",
      "(439, 345, 7, 1)\n",
      "(576, 72, 2, 1)\n",
      "(742, 434, 1, 1)\n",
      "(336, 295, 3, 1)\n",
      "(339, 295, 2, 1)\n",
      "(364, 0, 1, 4)\n",
      "(443, 339, 3, 1)\n",
      "(593, 19, 2, 3)\n",
      "(590, 16, 1, 1)\n",
      "(589, 19, 1, 1)\n",
      "(769, 241, 3, 1)\n",
      "(336, 22, 1, 1)\n",
      "(325, 199, 3, 1)\n",
      "(734, 427, 1, 1)\n",
      "(583, 0, 1, 11)\n",
      "(699, 240, 3, 1)\n",
      "(339, 16, 1, 1)\n",
      "(677, 240, 24, 1)\n",
      "(508, 215, 1, 1)\n",
      "(733, 426, 1, 1)\n",
      "(725, 416, 1, 1)\n",
      "(770, 242, 2, 1)\n",
      "(798, 243, 2, 1)\n",
      "(329, 18, 1, 1)\n",
      "(328, 19, 1, 1)\n",
      "(484, 210, 2, 1)\n",
      "(479, 209, 2, 1)\n",
      "(548, 16, 2, 1)\n",
      "(526, 45, 1, 3)\n",
      "(405, 97, 1, 1)\n",
      "(545, 0, 1, 5)\n",
      "(448, 342, 2, 1)\n",
      "(545, 0, 1, 5)\n",
      "(536, 228, 1, 2)\n",
      "(315, 214, 1, 1)\n",
      "(719, 241, 5, 1)\n",
      "(555, 19, 1, 1)\n",
      "(478, 330, 1, 1)\n",
      "(725, 411, 4, 4)\n",
      "(316, 21, 2, 1)\n",
      "(726, 412, 1, 1)\n",
      "(508, 216, 5, 1)\n",
      "(312, 210, 1, 1)\n",
      "(421, 130, 1, 1)\n",
      "(638, 239, 8, 1)\n",
      "(555, 0, 1, 14)\n",
      "(535, 233, 2, 2)\n",
      "(728, 414, 3, 3)\n",
      "(545, 364, 2, 1)\n",
      "(543, 281, 1, 1)\n",
      "(669, 240, 27, 1)\n",
      "(603, 238, 11, 1)\n",
      "(308, 21, 1, 1)\n",
      "(310, 19, 2, 2)\n",
      "(534, 229, 1, 1)\n",
      "(629, 239, 17, 1)\n",
      "(315, 215, 1, 1)\n",
      "(315, 215, 1, 1)\n",
      "(608, 238, 6, 1)\n",
      "(560, 55, 1, 2)\n",
      "(560, 56, 1, 1)\n",
      "(560, 56, 1, 1)\n",
      "(560, 56, 1, 1)\n",
      "(637, 239, 6, 1)\n",
      "(630, 239, 14, 1)\n",
      "(561, 79, 1, 2)\n",
      "(629, 239, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "confidence_score_threshold = 0.7\n",
    "\n",
    "# Open video capture (0 is the default camera)\n",
    "video_path = \"./passport_video.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "id_images_batch, id_labels = retrieve_saved_ids()\n",
    "frame_counter = 0  # Use local variable instead of global\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps == 0:  # Avoid division by zero\n",
    "    fps = 30  # Default to 30 FPS if unable to retrieve\n",
    "frame_delay = int(1000 / fps)  # Calculate frame delay in milliseconds\n",
    "print(frame_delay)\n",
    "\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"ID Detection\")\n",
    "window.geometry(\"1200x800\")\n",
    "\n",
    "\n",
    "# Create a label to display the video feed\n",
    "video_label = tk.Label(window)\n",
    "video_label.pack()\n",
    "\n",
    "# Function to update frame in Tkinter window\n",
    "def update_frame():\n",
    "    global frame_counter, id_images_batch, id_labels\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        window.after(10, update_frame)  # Schedule next frame update\n",
    "        return\n",
    "\n",
    "    \n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "    \n",
    "    # # Predict bounding box coordinates\n",
    "    # bounding_box_prediction = yolo(yolo_model, frame)\n",
    "\n",
    "    # # Make siamese network work only if ID card is found\n",
    "    # if bounding_box_prediction.xyxy[0].shape != (0, 6):\n",
    "    #     # Get coordinates of bounding box\n",
    "    #     x1, y1, x2, y2, confidence_score = get_results_of_yolo(bounding_box_prediction)\n",
    "\n",
    "    #     # Crop the ID card\n",
    "    #     cropped_id_card = crop_id_card(bounding_box_prediction, frame)\n",
    "\n",
    "    #     # Check if a new ID card is detected\n",
    "\n",
    "    \n",
    "    #     # Predict with siamese network\n",
    "    #     prediction_idx, confidence_score_siamese = predict(id_images_batch, cropped_id_card, model, threshold=0.95)\n",
    "\n",
    "    #     if prediction_idx == 999:\n",
    "    #         prediction_name = \"not found\"\n",
    "    #         if check_for_new_id_type(frame,confidence_score) == True:\n",
    "    #             id_images_batch, id_labels = retrieve_saved_ids()\n",
    "                \n",
    "    #     else:\n",
    "    #         prediction_name = id_labels[prediction_idx]\n",
    "\n",
    "    #     cv2.putText(frame, f'id type: {prediction_name}, {confidence_score_siamese:.2f}', (10, 30),\n",
    "    #                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "    #     if confidence_score > 0.45:\n",
    "    #         # Draw bounding box on the frame\n",
    "    #         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    #         text = f'{confidence_score:.2f}'\n",
    "    #         text_position = (x1, y1 - 10)\n",
    "    #         cv2.putText(frame, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    mrz_area, roi = find_mrz_area(frame)\n",
    "\n",
    "    if mrz_area is not None:\n",
    "        x, y, w, h = mrz_area\n",
    "        print(mrz_area)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        # cv2.imshow('MRZ Area', frame)\n",
    "        if roi is not None:\n",
    "            # print(roi)\n",
    "            if roi.shape[0]>0 and roi.shape[1]>0:\n",
    "                cv2.imshow('MRZ ROI', roi)\n",
    "    else:\n",
    "        print(\"MRZ area not found.\")\n",
    "    # Convert frame to RGB and update the Tkinter label\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    video_label.after(frame_delay, update_frame)\n",
    "\n",
    "# Start updating frames\n",
    "update_frame()\n",
    "\n",
    "# Define a function to handle window close\n",
    "def on_closing():\n",
    "    cap.release()  # Release the camera\n",
    "    window.destroy()  # Close the Tkinter window\n",
    "\n",
    "# Bind the window close event\n",
    "window.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7594675-7981-455a-8424-9f667395fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./spanish_id_front.jpg')\n",
    "\n",
    "def find_face(image):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    return image\n",
    "image = find_face(image)\n",
    "image = cv2.resize(image,(960,540))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3fe572f5-4fbf-423e-8c73-3ce29b566a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 406\n",
      "1152 2048\n",
      "2.3575676812065973\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('./spanish_id_front.jpg')\n",
    "\n",
    "rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 47))\n",
    "\n",
    "# Apply blackhat morphology\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "\n",
    "# Threshold and find contours\n",
    "_, thresh = cv2.threshold(blackhat, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter contours for signature area\n",
    "height, width, dim = image.shape\n",
    "for c in contours:\n",
    "    x, y, w, h = cv2.boundingRect(c)\n",
    "    if w > 200 and h > 100:\n",
    "        aspect_ratio = w / float(h)\n",
    "\n",
    "        if 1.5 < aspect_ratio < 4.5: \n",
    "            if 2 < ((w*h) * 100)/(height * width) < 5: # Signature is often wide\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "image = cv2.resize(image,(960,540))\n",
    "cv2.imshow(\"lala\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cff967-4795-440a-b7f5-9effa9e155ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005db7f6-1732-4480-b5e6-380fe81e74ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b08fc21-33dc-4ee0-aa89-a091b4a19317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "1.5285714285714285\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5b2897a-a0e3-41b2-a281-61db6a7645fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1551246537396125"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bc85f-4d57-4580-8c63-a621c66e75ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
